{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPOE6JflTqqxaR8GKxn0/g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TWOCHE/Data_Analytics_Tools/blob/master/comparing_CNN_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrVqWuq-DXU5"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-contrib-python-headless\n",
        "!pip install seaborn\n",
        "!pip3 install mlnotify"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from shutil import copy\n",
        "from shutil import copytree, rmtree\n",
        "import mlnotify\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "import cv2\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "from tensorflow.keras.applications import InceptionV3, VGG16, ResNet50\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n"
      ],
      "metadata": {
        "id": "J56Dd1HdDhib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download dataset\n",
        "!mkdir ./data\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://data.mendeley.com/public-files/datasets/rscbjbr9sj/files/f12eaf6d-6023-432f-acc9-80c9d7393433/file_downloaded \\\n",
        "    -O ./data/x-ray.zip"
      ],
      "metadata": {
        "id": "MCY3vJ2CDhln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract from archive\n",
        "\n",
        "!unzip -q ./data/x-ray.zip -d ./data/"
      ],
      "metadata": {
        "id": "BGqsputIDhqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the extracted dataset folder\n",
        "!ls ./data/chest_xray/"
      ],
      "metadata": {
        "id": "QznPKL8yDhs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleanup\n",
        "print('Removing unneeded folders and files...')\n",
        "! rm -rf ./data/chest_xray/.DS_Store\n",
        "! rm -rf ./data/chest_xray/train/.DS_Store\n",
        "! rm -rf ./data/chest_xray/test/.DS_Store\n",
        "! rm ./data/x-ray.zip\n",
        "shutil.rmtree('./data/__MACOSX/')"
      ],
      "metadata": {
        "id": "Fnh29ShuDhvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls -l ./data/chest_xray\n",
        "\n",
        "! ls -l ./data/chest_xray/train\n",
        "\n",
        "! ls -l ./data/chest_xray/test"
      ],
      "metadata": {
        "id": "yI7lQXyyDhyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataset_dir = './data/chest_xray/'\n",
        "\n",
        "train_dir = os.path.join(dataset_dir, 'train')\n",
        "test_dir = os.path.join(dataset_dir, 'test')\n",
        "\n",
        "# Get a list of class names\n",
        "class_names = os.listdir(train_dir)\n",
        "\n",
        "def print_class_info(directory, dataset_type):\n",
        "    print(f'Number of x-ray image classes in {dataset_type}:', len(class_names))\n",
        "    print(f'{dataset_type} class names:', class_names)\n",
        "    for cls in class_names:\n",
        "        class_path = os.path.join(directory, cls)\n",
        "        num_images = len(os.listdir(class_path))\n",
        "        print(f'{cls} : {num_images}')\n",
        "\n",
        "print_class_info(train_dir, 'train')\n",
        "print(\"\\n\")\n",
        "print_class_info(test_dir, 'test')"
      ],
      "metadata": {
        "id": "16ukPeu3ESqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's see what the filenames look like\n",
        "print(os.listdir(train_dir + '/' + 'NORMAL')[:5])\n",
        "print(os.listdir(train_dir + '/' + 'PNEUMONIA')[:5])"
      ],
      "metadata": {
        "id": "th_DJK5eEStN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize one image from each class - ['NORMAL', 'PNEUMONIA']\n",
        "npic = './data/chest_xray/train/NORMAL/NORMAL2-IM-0517-0001.jpeg'\n",
        "ppic = './data/chest_xray/train/PNEUMONIA/person1265_virus_2156.jpeg'\n",
        "\n",
        "plt.figure(1, figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(cv2.imread(npic))\n",
        "plt.title(\"NORMAL x-ray\")\n",
        "plt.xticks([]) , plt.yticks([])\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(cv2.imread(ppic))\n",
        "plt.title(\"PNEUMONIA x-ray\")\n",
        "plt.xticks([]) , plt.yticks([])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KIfiXbOOESwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "COUNT_NORMAL = 1349\n",
        "COUNT_PNEUMONIA = 3884\n",
        "\n",
        "initial_bias = np.log([COUNT_PNEUMONIA / COUNT_NORMAL])\n",
        "print(\"Initial bias: {:.5f}\".format(initial_bias[0]))\n",
        "\n",
        "TRAIN_IMG_COUNT = COUNT_NORMAL + COUNT_PNEUMONIA\n",
        "weight_for_0 = (1 / COUNT_NORMAL) * (TRAIN_IMG_COUNT) / 2.0\n",
        "weight_for_1 = (1 / COUNT_PNEUMONIA) * (TRAIN_IMG_COUNT) / 2.0\n",
        "\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
        "\n",
        "print(\"Weight for class 0: {:.2f}\".format(weight_for_0))\n",
        "print(\"Weight for class 1: {:.2f}\".format(weight_for_1))"
      ],
      "metadata": {
        "id": "TdF5_bGAEhiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "\n",
        "input_size = 299\n",
        "batch_size = 32\n",
        "learning_rate = 0.0005\n",
        "size_inner = 128\n",
        "droprate = 0.2\n",
        "n_epochs = 20"
      ],
      "metadata": {
        "id": "31BfJGViEhlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data generatores\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    shear_range=10,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2     # set validation split\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input\n",
        ")\n",
        "\n",
        "# Flow training images in batches using train_datagen generator\n",
        "train_ds = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(input_size, input_size),\n",
        "        batch_size=batch_size,\n",
        "        subset='training'\n",
        ")\n",
        "\n",
        "# Flow val images in batches using val_datagen generator\n",
        "val_ds = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(input_size, input_size),\n",
        "        batch_size=batch_size,\n",
        "        subset='validation'\n",
        ")\n",
        "\n",
        "# Flow test images in batches using test_datagen generator\n",
        "test_ds = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(input_size, input_size),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "fhfnoVCAEhnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at 1 of the the batches\n",
        "train_batch = train_ds[0]\n",
        "images, labels = list(train_batch)\n",
        "print(images.shape)\n",
        "print(labels.shape)\n",
        "\n",
        "first_image = images[0]\n",
        "first_image"
      ],
      "metadata": {
        "id": "iHsfNxqSEhp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_image[:3, :3, 0]"
      ],
      "metadata": {
        "id": "ig76IewMExaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch = train_ds\n",
        "train_images, train_labels = list(train_batch)"
      ],
      "metadata": {
        "id": "ZCSCB6vXC144"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_images.shape)"
      ],
      "metadata": {
        "id": "nvIkek2m5k6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_batch = test_ds[0]\n",
        "test_images, test_labels = list(test_batch)"
      ],
      "metadata": {
        "id": "lVevpQYmFPht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Function to create and compile a CNN model\n",
        "def create_and_evaluate_cnn_model(model_name, input_size=299, learning_rate=0.001, size_inner=512, droprate=0.2, num_classes=2):\n",
        "    # Define the available CNN models\n",
        "    available_models = {\n",
        "        \"InceptionV3\": InceptionV3,\n",
        "        \"VGG16\": VGG16,\n",
        "        \"ResNet50\": ResNet50\n",
        "    }\n",
        "\n",
        "    # Check if the specified model_name is valid\n",
        "    if model_name not in available_models:\n",
        "        raise ValueError(f\"Invalid model name. Supported models: {', '.join(available_models.keys())}\")\n",
        "\n",
        "    # Load the selected pre-trained model\n",
        "    base_model = available_models[model_name](\n",
        "        weights='imagenet',\n",
        "        include_top=False,\n",
        "        input_shape=(input_size, input_size, 3)\n",
        "    )\n",
        "\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Define the custom classification head\n",
        "    inputs = keras.Input(shape=(input_size, input_size, 3))\n",
        "    base = base_model(inputs, training=False)\n",
        "    vectors = keras.layers.GlobalAveragePooling2D()(base)\n",
        "\n",
        "    inner = keras.layers.Dense(size_inner, activation='relu')(vectors)\n",
        "    drop = keras.layers.Dropout(droprate)(inner)\n",
        "\n",
        "    outputs = keras.layers.Dense(num_classes)(drop)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    loss = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=loss,\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "JtY4BPi0B730"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train and evaluate a model\n",
        "def train_and_evaluate_model(model_name, train_data, test_data, epochs=10, batch_size=32):\n",
        "    # Create and compile the model\n",
        "    model = create_and_evaluate_cnn_model(model_name)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(train_data[0], epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "    # Evaluate the model on test data\n",
        "    test_images, test_labels = test_data\n",
        "    predictions = model.predict(test_images)\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "    true_labels = np.argmax(test_labels, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    confusion = confusion_matrix(true_labels, predicted_labels)\n",
        "    report = classification_report(true_labels, predicted_labels)\n",
        "\n",
        "    return accuracy, confusion, report\n",
        "\n",
        "# Specify your model names, input data, and hyperparameters\n",
        "model_names = [\"InceptionV3\", \"VGG16\", \"ResNet50\"]\n",
        "input_size = 299\n",
        "lr_schedule = 0.001\n",
        "size_inner = 512\n",
        "droprate = 0.2\n",
        "\n",
        "# Create an empty dictionary to store the models and their results\n",
        "models = {}\n",
        "\n",
        "# Iterate through each model\n",
        "for model_name in model_names:\n",
        "    # Train and evaluate the model, and store the results in a dictionary\n",
        "    accuracy, confusion, report = train_and_evaluate_model(\n",
        "        model_name,\n",
        "        (train_images, train_labels),\n",
        "        (test_images, test_labels),\n",
        "        epochs=10,\n",
        "        batch_size=32\n",
        "    )\n",
        "\n",
        "    models[model_name] = {\n",
        "        \"model\": model_name,\n",
        "        \"accuracy\": accuracy,\n",
        "        \"confusion_matrix\": confusion.tolist(),  # Convert the NumPy array to a list\n",
        "        \"classification_report\": report\n",
        "    }\n",
        "\n",
        "# Now 'models' contains the evaluated models and their results\n",
        "\n"
      ],
      "metadata": {
        "id": "6r7G5fNNZ9Bu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models"
      ],
      "metadata": {
        "id": "lVmdGhDKFrKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checkpoint callback\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "    \"xray_model.h5\",\n",
        "    save_best_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max'\n",
        ")\n",
        "\n",
        "# Defining early stopping to prevent overfitting\n",
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor = 'val_loss',\n",
        "    mode = 'auto',\n",
        "    min_delta = 0,\n",
        "    patience = 2,\n",
        "    verbose = 2,\n",
        "    restore_best_weights = True\n",
        ")"
      ],
      "metadata": {
        "id": "D76S4pi9dnHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exponential learning rate decay\n",
        "initial_learning_rate = 0.015\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
        ")"
      ],
      "metadata": {
        "id": "pODwsu3EdnFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict on test data\n",
        "predictions=[]\n",
        "for model_name in model_names:\n",
        "  test_ds.reset()\n",
        "  preds = model_name.predict(test_ds, verbose=2)\n",
        "  predicted_class_indices=np.argmax(preds,axis=1)\n",
        "  labels = (train_ds.class_indices)\n",
        "  labels = dict((v,k) for k,v in labels.items())\n",
        "  predictions = [labels[k] for k in predicted_class_indices]"
      ],
      "metadata": {
        "id": "bJEbvb-0kYGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filenames=test_ds.filenames\n",
        "results=pd.DataFrame({\"Filename\":filenames,\n",
        "                      \"Predictions\":predictions})\n",
        "\n",
        "results[:-3]"
      ],
      "metadata": {
        "id": "UvtXdsFiExiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name in model_names:\n",
        "  path = './data/chest_xray/test/PNEUMONIA/person104_bacteria_492.jpeg'\n",
        "  img = load_img(path, target_size=(299, 299))\n",
        "  x = tf.keras.preprocessing.image.img_to_array(img)\n",
        "  X = np.array([x])  # Convert a single image to a batch.\n",
        "  X = preprocess_input(X)  # Use the preprocessing function for InceptionV3\n",
        "  prediction = model_name.predict(X)[0].flatten()\n",
        "  prediction = (prediction - np.min(prediction)) / np.ptp(prediction)\n",
        "  print({class_names[i]: float(prediction[i]) for i in range(2)})\n",
        "  print(\"Prediction completed for \",model_name)"
      ],
      "metadata": {
        "id": "piX52A5eDh0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FZk1h2Rxujh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sc_Awxbjujeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dXFH6Sqhuja8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b11s7l_EujXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tQ9-ie8SDh2z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}